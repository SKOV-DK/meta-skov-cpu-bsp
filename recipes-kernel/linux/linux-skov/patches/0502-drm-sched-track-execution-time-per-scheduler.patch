From: Lucas Stach <l.stach@pengutronix.de>
Date: Fri, 8 Jun 2018 12:10:40 +0200
Subject: [PATCH] drm/sched: track execution time per scheduler

This will alllow us to export some useful statistics to userspace.
A next step would be to track execution time per job/client, but this
needs stronger ordering on the fence completion than what is currently
provided and it's unclear if all drivers can provide the necessary
ordering.

Signed-off-by: Lucas Stach <l.stach@pengutronix.de>
---
 drivers/gpu/drm/scheduler/sched_main.c | 13 +++++++++++++
 1 file changed, 13 insertions(+)

diff --git a/drivers/gpu/drm/scheduler/sched_main.c b/drivers/gpu/drm/scheduler/sched_main.c
index e82897a4e819..95a9e89a0428 100644
--- a/drivers/gpu/drm/scheduler/sched_main.c
+++ b/drivers/gpu/drm/scheduler/sched_main.c
@@ -274,6 +274,11 @@ static void drm_sched_job_begin(struct drm_sched_job *s_job)
 	spin_lock(&sched->job_list_lock);
 	list_add_tail(&s_job->node, &sched->ring_mirror_list);
 	drm_sched_start_timeout(sched);
+
+	if (list_is_singular(&sched->ring_mirror_list)) {
+		sched->stats->active_ts = ktime_get();
+		sched->stats->active = true;
+	}
 	spin_unlock(&sched->job_list_lock);
 }
 
@@ -668,6 +673,8 @@ static void drm_sched_process_job(struct dma_fence *f, struct dma_fence_cb *cb)
 static struct drm_sched_job *
 drm_sched_get_cleanup_job(struct drm_gpu_scheduler *sched)
 {
+	struct drm_gpu_scheduler_stats *stats = sched->stats;
+	ktime_t now = ktime_get();
 	struct drm_sched_job *job;
 
 	/*
@@ -695,6 +702,12 @@ drm_sched_get_cleanup_job(struct drm_gpu_scheduler *sched)
 
 	spin_unlock(&sched->job_list_lock);
 
+	spin_lock(&stats->lock);
+	stats->active_time_us += ktime_to_us(ktime_sub(now, stats->active_ts));
+	stats->active_ts = now;
+	stats->active = false;
+	spin_unlock(&stats->lock);
+
 	return job;
 }
 
