From: Lucas Stach <l.stach@pengutronix.de>
Date: Fri, 8 Jun 2018 12:10:40 +0200
Subject: [PATCH] drm/sched: track execution time per scheduler

This will alllow us to export some useful statistics to userspace.
A next step would be to track execution time per job/client, but this
needs stronger ordering on the fence completion than what is currently
provided and it's unclear if all drivers can provide the necessary
ordering.

Signed-off-by: Lucas Stach <l.stach@pengutronix.de>
---
 drivers/gpu/drm/scheduler/sched_main.c | 14 ++++++++++++++
 1 file changed, 14 insertions(+)

diff --git a/drivers/gpu/drm/scheduler/sched_main.c b/drivers/gpu/drm/scheduler/sched_main.c
index 0964c9613e9b..49b13629ad48 100644
--- a/drivers/gpu/drm/scheduler/sched_main.c
+++ b/drivers/gpu/drm/scheduler/sched_main.c
@@ -311,6 +311,11 @@ static void drm_sched_job_begin(struct drm_sched_job *s_job)
 	spin_lock(&sched->job_list_lock);
 	list_add_tail(&s_job->list, &sched->pending_list);
 	drm_sched_start_timeout(sched);
+
+	if (list_is_singular(&sched->pending_list)) {
+		sched->stats->active_ts = ktime_get();
+		sched->stats->active = true;
+	}
 	spin_unlock(&sched->job_list_lock);
 }
 
@@ -825,6 +830,7 @@ drm_sched_select_entity(struct drm_gpu_scheduler *sched)
 static struct drm_sched_job *
 drm_sched_get_cleanup_job(struct drm_gpu_scheduler *sched)
 {
+	struct drm_gpu_scheduler_stats *stats = sched->stats;
 	struct drm_sched_job *job, *next;
 
 	spin_lock(&sched->job_list_lock);
@@ -858,6 +864,14 @@ drm_sched_get_cleanup_job(struct drm_gpu_scheduler *sched)
 		job->entity->elapsed_ns += ktime_to_ns(
 			ktime_sub(job->s_fence->finished.timestamp,
 				  job->s_fence->scheduled.timestamp));
+		spin_lock(&stats->lock);
+		stats->active_time_us +=
+			ktime_to_us(ktime_sub(job->s_fence->finished.timestamp,
+					      job->s_fence->scheduled.timestamp));
+		stats->active_ts = job->s_fence->finished.timestamp;
+		if (!next)
+			stats->active = false;
+		spin_unlock(&stats->lock);
 	}
 
 	return job;
